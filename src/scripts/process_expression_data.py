import pandas as pd
import argparse


def expression_file_loader(included_ids, cutoff):
    """Function to load the raw expression data in chunks, discarding anything we don't need."""
    # get headers from expression_data file
    expression_data = pd.read_csv("tmp/raw_expression_data.tsv", sep='\t', nrows=0)
    # read expression_data in chunks of 1000 rows
    expression_reader = pd.read_csv("tmp/raw_expression_data.tsv", sep='\t', dtype={'id': 'category', 'gene': 'category'}, chunksize=1000)
    # filter each chunk and concatenate
    for chunk in expression_reader:
        filtered = chunk[(chunk['expression_extent']>cutoff) & (chunk['id'].isin(included_ids))]
        expression_data = pd.concat([expression_data, filtered])
    return expression_data
    

if __name__ == "__main__":
    # run the expression processing only if this is run rather than imported
    
    # can specify whether to regenerate tsvs for all existing datasets.
    # New datasets will always be generated.
    parser = argparse.ArgumentParser()
    parser.add_argument("-r", "--refresh", help="refresh all existing metadata",
                        action="store_true")
    args = parser.parse_args()

    expression_cutoff = 0.2

    # excluded clusters and existing entities
    all_inclusions = []
    with open('tmp/all_inclusions.txt', 'r') as file: # generated by process_metadata
        for line in file:
            all_inclusions.append(line.rstrip())

    existing_entities = []
    if not args.refresh:
        with open('tmp/internal_terms.txt', 'r') as file: # all FBlcs in owl files
            for line in file:
                existing_entities.append("FlyBase:" + line.rstrip())

    included_entities_to_update = [e for e in all_inclusions if not (e in existing_entities)]

    # EXPRESSION DATA

    # make dict of cluster:dataset
    cluster_metadata = pd.read_csv("tmp/raw_cluster_data.tsv", sep='\t').set_index('id')
    dataset_cluster_dict = {c: cluster_metadata['associated_dataset'][c] for c in cluster_metadata.index.values}

    expression_data = expression_file_loader(included_entities_to_update, expression_cutoff)

    # make a a tsv for each new cluster
    clusters = expression_data['id'].drop_duplicates()
    print(str(len(clusters)) + ' clusters')
    for c in clusters:
        cluster_data = expression_data[expression_data['id']==c]
        cluster_data = cluster_data.assign(hide_in_terminfo = 'true')
        cluster_id = c.replace("FlyBase:", "")
        cluster_data.to_csv("expression_data/dataset_%s-cluster_%s.tsv" % (dataset_cluster_dict[c].replace("FlyBase:", ""), cluster_id), sep='\t', index=False)
